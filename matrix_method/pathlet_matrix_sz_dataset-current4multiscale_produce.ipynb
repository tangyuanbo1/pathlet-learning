{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d47b9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "from utils.utils import *\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import dill\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "# print(torch.__version__)\n",
    "# print(torch.version.cuda)\n",
    "# use_gpu = torch.cuda.is_available()\n",
    "# use_gpu\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from tqdm import tqdm\n",
    "target = \"5_4\"\n",
    "# target = \"5_5\"\n",
    "# target = \"p2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7898dfc2",
   "metadata": {},
   "source": [
    "获得同时经过两个区域的轨迹的id以及在各自区域对应的索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09718461",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6364 6364 6364 4000\n"
     ]
    }
   ],
   "source": [
    "ids_5_5 = np.load(\"/root/multiscale/tra_id_listfutian5_5_10685.npy\",allow_pickle=True).tolist()\n",
    "ids_5_4 = np.load(\"/root/multiscale/tra_id_listfutian5_4_39895.npy\",allow_pickle=True).tolist()\n",
    "ids_p2 = np.load(\"/root/multiscale/tra_id_futian_p2_list0613.npy\",allow_pickle=True).tolist()\n",
    "\n",
    "ids_common = np.load(\"/root/multiscale/common_tra_id_listfutian6364.npy\",allow_pickle=True).tolist()\n",
    "index_list_5_4=[]\n",
    "index_list_5_5=[]\n",
    "index_list_p2=[]\n",
    "for id in ids_common:\n",
    "    index_list_5_4.append(  ids_5_4.index(id))\n",
    "    index_list_5_5.append(  ids_5_5.index(id))\n",
    "    if id in ids_p2:\n",
    "        index_list_p2.append(  ids_p2.index(id))\n",
    "print(len(ids_common),len(index_list_5_4),len(index_list_5_5),len(index_list_p2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b3bea3-9a45-40d4-b6e3-92f5ab314d95",
   "metadata": {},
   "source": [
    "数据初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4d66bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of raw tras when loading: 39895\n",
      "len of raw tras after filter edges more than 6: 39895\n",
      "the first trajectory:\n",
      "[(7902654987, 6020564585), (6020564585, 4138689909), (4138689909, 9660848465), (9660848465, 9660848466), (9660848466, 7868781069), (7868781069, 9660848467), (9660848467, 9660848480), (9660848480, 9660848481), (9660848481, 9660848469), (9660848469, 9660848464), (9660848464, 9660848463), (9660848463, 6020564517), (6020564517, 9660848462), (9660848462, 6020564510), (6020564510, 9608302600), (9608302600, 2269583663), (2269583663, 2269583676)]\n",
      "split tras and tras_test\n",
      "tras chosen: 39895\n",
      "len of raw tras after filter: 4000\n"
     ]
    }
   ],
   "source": [
    "# 设置基础参数和文件路径\n",
    "l = 10000 # 边的数量\n",
    "m = 30000 # p空间的数量\n",
    "n = 300 # 轨迹数据集的大小\n",
    "# 超参数\n",
    "mu=10\n",
    "la=10.0/n\n",
    "if target==\"5_4\":\n",
    "    raw_tras_name = \"tra_datasetfutian5_4_39895\"\n",
    "if target==\"5_5\":\n",
    "    raw_tras_name = \"tra_datasetfutian5_5_10685\"\n",
    "if target==\"p2\":\n",
    "    raw_tras_name = \"tra_dataset_futian_p2_40000613\"\n",
    "\n",
    "\n",
    "raw_tras_sz = load_tras(\"/root/multiscale/\"+raw_tras_name+\".npy\")\n",
    "print(\"len of raw tras when loading:\",len(raw_tras_sz))\n",
    "tras,tra_index_list = filter_raw_num_of_edge_6(raw_tras_sz)\n",
    "# np.save('tra_index_list.npy',np.array(tra_index_list,dtype=object))\n",
    "if target==\"5_4\":\n",
    "    tras = [tras[index] for index in index_list_5_4]\n",
    "if target==\"5_5\":\n",
    "    tras = [tras[index] for index in index_list_5_5]\n",
    "if target==\"p2\":\n",
    "    tras = [tras[index] for index in index_list_p2]\n",
    "\n",
    "tras = tras[:4000] \n",
    "print(\"len of raw tras after filter:\",len(tras))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac658a96-500c-4e2e-8ba5-e716f6263046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(edge_list): 104466 len(path_list): 571195 len(tras): 4000\n",
      "len(edge_list): 3866 len(path_list): 271457 len(tras): 4000\n",
      "len(edge_list): 3866 len(path_list): 45086 len(tras): 4000\n"
     ]
    }
   ],
   "source": [
    "# 我们需要获得所有的edge并且编号；并且获得所有的t与e的对应关系 这样方便下一步计算矩阵，也就是得到edge_list and path_list\n",
    "# 先计算一下e和p的大小\n",
    "edge_list = []\n",
    "path_list = []\n",
    "path_list_cnt_ = []\n",
    "tra_index = 0\n",
    "def generate_all_p_(t,min,max):\n",
    "    res=[]\n",
    "    for i in range(len(t)):\n",
    "        for j in range(i+1,len(t)+1):\n",
    "            if ((j-i)>min) and ((j-i)<max):\n",
    "                res.append(t[i:j])\n",
    "    return res\n",
    "# path_list_dict={}#记录不同的pathlet\n",
    "path_list_dict_cnt={}#记录不同的pathlet对应的被经过的次数\n",
    "for i in range(len(tras)):\n",
    "    tra = tras[i]\n",
    "    for edge in tra:\n",
    "        edge_list.append(str(edge).replace(\" \",\"\")) \n",
    "    path_min=12\n",
    "    path_max=30\n",
    "    if target==\"p2\":\n",
    "        path_min=1\n",
    "        path_max=30\n",
    "    for path in generate_all_p_(tra,min=path_min,max=path_max):\n",
    "        path_len = len(path)\n",
    "        # if path_len>10:\n",
    "        #     print(path_len)\n",
    "        path_str = str(path).replace(\" \",\"\")\n",
    "        path_list.append(path_str)\n",
    "        if path_str not in path_list_dict_cnt.keys():\n",
    "            path_list_dict_cnt[path_str]=[1,0]\n",
    "        else:\n",
    "            path_list_dict_cnt[path_str][0]+=1\n",
    "l,m,n = len(edge_list)+10,len(path_list)+10,len(tras)\n",
    "print(\"len(edge_list):\",l,\"len(path_list):\",m,\"len(tras):\",n)\n",
    "edge_list = list(set(edge_list))           \n",
    "path_list = list(path_list_dict_cnt.keys())\n",
    "l,m,n = len(edge_list)+10,len(path_list)+10,len(tras)\n",
    "print(\"len(edge_list):\",l,\"len(path_list):\",m,\"len(tras):\",n)\n",
    "# 这一次是根据pathlet的frequency来过滤一部分不太常用的path\n",
    "filter_num=1\n",
    "if target==\"p2\":\n",
    "    filter_num=0\n",
    "old_path_list = path_list\n",
    "path_list = []\n",
    "path_index=0\n",
    "for i in range(len(old_path_list)):\n",
    "    if path_list_dict_cnt[old_path_list[i]][0]>filter_num:\n",
    "        path_list.append(old_path_list[i])\n",
    "        path_list_dict_cnt[old_path_list[i]][1]=path_index\n",
    "        path_index+=1\n",
    "# for i in trange(len(tras)):\n",
    "#     tra = tras[i]\n",
    "#     path_list.append(str(tra).replace(\" \",\"\"))\n",
    "l,m,n = len(edge_list)+10,len(path_list)+10,len(tras)\n",
    "print(\"len(edge_list):\",l,\"len(path_list):\",m,\"len(tras):\",n)\n",
    "\n",
    "# # 存储pathlet_list\n",
    "np.save('path_list'+raw_tras_name+'.npy',np.array(path_list,dtype=object))   # 保存为.npy格式\n",
    "# np.save('edge_list.npy',np.array(edge_list,dtype=object))   # 保存为.npy格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d73eed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [00:04<00:00, 870.30it/s]\n",
      "100%|██████████| 45076/45076 [01:49<00:00, 412.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.dtype:  torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 计算三个矩阵19868742712\n",
    "D = np.zeros((l, m)) #记录e与p之间关系\n",
    "R = np.zeros(( m,n)) #记录p与t之间关系\n",
    "T = np.zeros((l, n)) #记录e和t之间关系\n",
    "D_tensor=torch.zeros(len(edge_list)+10,len(path_list)+10)#.cuda()\n",
    "computed_path = []\n",
    "\n",
    "tra_index = 0\n",
    "for i in trange(len(tras)):\n",
    "    tra = tras[i]\n",
    "    for edge in tra:\n",
    "        T[edge_list.index(str(edge).replace(\" \",\"\"))][i] = 1\n",
    "    # path_index = path_list.index(str(tra).replace(\" \",\"\"))\n",
    "    # R[path_index][i]=1\n",
    "from tqdm import tqdm\n",
    "for path in tqdm(path_list):\n",
    "    path_index = path_list.index(path)\n",
    "    edge_list_of_temp_pathlet= path_str2list(path)\n",
    "    for edge in edge_list_of_temp_pathlet:\n",
    "        if edge in edge_list:\n",
    "            D_tensor[edge_list.index(edge)][path_index] = 1\n",
    "\n",
    "D_tensor=D_tensor#.cuda()\n",
    "T_tensor=torch.from_numpy(T[:len(edge_list)+10])#.cuda()\n",
    "print('x.dtype: ',D_tensor.dtype)  # x的具体类型\n",
    "R_tensor_tejie=torch.from_numpy(R[:len(path_list)+10])\n",
    "# R_tensor=torch.rand(len(path_list)+10,n).double()\n",
    "# D=np.load(\"./D.npy\",allow_pickle=True)\n",
    "# T=np.load(\"./T.npy\",allow_pickle=True)\n",
    "# R=np.load(\"./R.npy\",allow_pickle=True)\n",
    "# # 保存数据为csv，供matlab使用\n",
    "# numpy.savetxt(\"D_tensor_148.csv\", D_tensor.cpu().detach().numpy(), delimiter=',')\n",
    "# numpy.savetxt(\"T_tensor_148.csv\", T_tensor[:,0:30].cpu().detach().numpy(), delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d1b0c2",
   "metadata": {},
   "source": [
    "# 训练开始"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa755d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "initial_R = [\n",
    "    # torch.rand(len(path_list)+10,n).double(), # random初始化\n",
    "torch.from_numpy(R[:len(path_list)+10]), #全不分割初始化\n",
    "torch.zeros( m,n) #全0初始化\n",
    "]\n",
    "\n",
    "LR = 0.005\n",
    "epoch_num =500\n",
    "final_loss = []\n",
    "R_list = []\n",
    "value_distribution_list = []\n",
    "R_tensor_nonzero_ratio = []\n",
    "subset=0\n",
    "num_of_subset=10000\n",
    "gamma_value=0.96\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92d6955e-6d69-4c6f-bdba-33777aea733f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------lambda_value= 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [03:41<00:00,  2.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_size 1118.699951171875\n",
      "representation_cost 7157.2998046875\n",
      "avg representation_cost 1.789324951171875\n",
      "num of all edges: 104087.0\n",
      "num of uncover edges: 6052.47\n",
      "num of overlap edges: 18699.705\n",
      "uncover ratio: 0.0581\n",
      "overlap ratio: 0.18\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mu_values = [0,0,0.81,0.2]\n",
    "# mu_values = [0,0,1.51,2]\n",
    "la_list = [0,0.1,0.3,0.7,1,5]\n",
    "la_list=[0.1]\n",
    "for la in la_list:\n",
    "    loss_his,R_tensor,muvalue = R_train(initial_R[0],D_tensor.float().cuda(),T_tensor.float().cuda(),\n",
    "subset,num_of_subset,la,mu_values[0],mu_values[1],mu_values[2],mu_values[3],epoch_num,LR,gamma_value)\n",
    "    final_loss.append(loss_his)\n",
    "    \n",
    "    report_performance(D_tensor.cuda(),R_tensor.cuda(),T_tensor.cuda())\n",
    "    R_list.append(copy.deepcopy(R_tensor.cpu().detach().numpy()))\n",
    "# 是否只使用一部分的字典来重构轨迹数据集\n",
    "# 如果需要的话 需要改subset的值为1 同时注释 for control_lambda in control_lambda_list:部分\n",
    "#  这样的得到的结果就是当保留num_of_subset这么多pathlets的时候的性能\n",
    "# subset=1\n",
    "# num_of_subset = [100,200,300,500,1000,1500,2000,3000,4000]\n",
    "# # if subset==1:\n",
    "# loss_subset_record = []\n",
    "# for i in num_of_subset:\n",
    "#     p_picked_index = list(pathlet_dictionary[\"index\"]) #这是被选中到字典中的p的index\n",
    "#     p_picked_index_subset = p_picked_index[:i]\n",
    "#     D_tensor_subset=(D_tensor[:len(edge_list)+10,:len(path_list)+10]).cuda()\n",
    "#     # D_tensor_test_zeros = copy.deepcopy(D_tensor)\n",
    "#     for j in range(D_tensor_subset.shape[1]):\n",
    "#         if j  not in p_picked_index_subset:\n",
    "#             D_tensor_subset[:,j]=0\n",
    "#     control_lambda=0.15*len(tras)\n",
    "#     la = 0.1#control_lambda/len(tras)\n",
    "#     loss_his_subset,R_tensor_subset,muvalue = R_train(initial_R[0],D_tensor_subset.float().cuda(),T_tensor.float().cuda()\n",
    "#                                               ,subset,i,la,mu_values[0],mu_values[1],mu_values[2],mu_values[3],epoch_num,LR,gamma_value)\n",
    "#     loss_subset_record.append(loss_his_subset)\n",
    "\n",
    "# diff datasets \n",
    "# diff lambda\n",
    "# diff theta   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d0fc25",
   "metadata": {},
   "source": [
    "# 开始随机rounding，输出的结果为R_tensor_rounding，并且获得字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c723e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "rounding procedure start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45086/45086 [01:23<00:00, 537.47it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rounding procedure end\n",
      "loss compute start\n",
      "dict_size 4666.0\n",
      "representation_cost 14762.0\n",
      "avg representation_cost 3.6905\n",
      "num of all edges: 104087.0\n",
      "num of uncover edges: 10980.0\n",
      "num of overlap edges: 145747.0\n",
      "uncover ratio: 0.1055\n",
      "overlap ratio: 1.4\n",
      "loss compute end\n",
      "compute dictionary start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45086/45086 [00:00<00:00, 520908.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute dictionary end\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "# R_list.append(copy.deepcopy(R_tensor.cpu().detach().numpy()))\n",
    "import math\n",
    "# 随机rounding\n",
    "from scipy.stats import bernoulli\n",
    "# wuwu_list = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0,1.1,1.2,1.3,1.4,1.5]\n",
    "wuwu_list = [0.8,0.9,1,1.1,1.2,1.3,1.4,1.5,1.6,1.7]\n",
    "wuwu_list =[1]\n",
    "# for wuwu in wuwu_list:\n",
    "cnt=0\n",
    "pathlet_dictionary_list=[]\n",
    "rounding_loss_under_las = []\n",
    "repeated_rounding_num=1\n",
    "rounded_R_tensor_list = [] \n",
    "iterate_cnt=0\n",
    "for R_data_load in R_list[:]:\n",
    "    print(iterate_cnt)\n",
    "    iterate_cnt+=1\n",
    "    rounding_loss=[[],[],[],[],[],[],[]]\n",
    "    print(\"rounding procedure start\")\n",
    "    if 1:\n",
    "        wuwu=1\n",
    "        wuwu=math.log(2*torch.norm(T_tensor,1).cpu().data.numpy().round(1))*0.2\n",
    "        R_data_star = copy.deepcopy(R_data_load)\n",
    "        # R_data = copy.deepcopy(R_data_load)\n",
    "        R_data = np.zeros((R_data_star.shape[0],R_data_star.shape[1],repeated_rounding_num))\n",
    "        for i in trange(R_data_star.shape[0]):\n",
    "            if R_data_star[i].sum()>0.1:\n",
    "                for j in range(R_data_star.shape[1]):\n",
    "                    if(R_data_star[i][j]>0.03):\n",
    "                        R_data[i][j][:] = bernoulli.rvs(size=repeated_rounding_num,p=min(1,wuwu*R_data_star[i][j]))\n",
    "                    else:\n",
    "                        R_data[i][j]=0\n",
    "        # .data = R_data_load\n",
    "    print(\"rounding procedure end\")\n",
    "    print(\"loss compute start\")\n",
    "    R_data_tensor = torch.from_numpy(R_data)\n",
    "    relu = torch.nn.ReLU().cuda()\n",
    "    if_subset=0\n",
    "    diff_norm_num=2\n",
    "    for ite in range(repeated_rounding_num):\n",
    "        R_tensor_rounding = R_data_tensor[:,:,ite].cuda().float()\n",
    "        # print(\"-------------------------------\")\n",
    "        # print(\"wuwu:\",wuwu)\n",
    "        report_performance(D_tensor.cuda(),R_tensor_rounding.cuda(),T_tensor.cuda())\n",
    "        # rounding_loss[0].append((a+b+c+d+e+f).cpu().data.numpy().round(1))\n",
    "        # rounding_loss[1].append(a.cpu().data.numpy().round(1))\n",
    "        # rounding_loss[2].append(b.cpu().data.numpy().round(1))\n",
    "        # rounding_loss[3].append(c.cpu().data.numpy().round(1))\n",
    "        # rounding_loss[4].append(d.cpu().data.numpy().round(1))\n",
    "        # rounding_loss[5].append(e.cpu().data.numpy().round(1))\n",
    "        # rounding_loss[6].append(f.cpu().data.numpy().round(1))\n",
    "    # rounding_loss_under_las.append(rounding_loss)\n",
    "    print(\"loss compute end\")\n",
    "    print(\"compute dictionary start\")\n",
    "# if 1:\n",
    "    # 根据R_tensor_rounding计算每一个pathlet被用到的次数，并查询得到每一个path的长度\n",
    "    path_list_len = len(path_list)\n",
    "    path_list_cnt = np.zeros((path_list_len+10,3))\n",
    "    R_size = R_tensor_rounding.size()\n",
    "    pathlet_cnt = torch.sum(R_tensor_rounding, dim=1).cpu().numpy().round(1)\n",
    "    for i in trange(R_size[0]):\n",
    "        path_list_cnt[i][0]=i\n",
    "        path_list_cnt[i][1]=pathlet_cnt[i]\n",
    "        if i <path_list_len:\n",
    "            path_list_cnt[i][2]=len(path_list[i])\n",
    "\n",
    "    # 获得字典\n",
    "    path_list_cnt_df = pd.DataFrame(path_list_cnt,columns=[\"index\",\"cnt\",'len'])\n",
    "    pathlet_dictionary = path_list_cnt_df[path_list_cnt_df[\"cnt\"]>0].sort_values(by=['cnt'],ascending = False)\n",
    "     \n",
    "    pathlet_dictionary_list.append(pathlet_dictionary)\n",
    "    print(\"compute dictionary end\")\n",
    "    rounded_R_tensor_list.append(R_data_tensor)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45b9859b-16c4-4b0e-90ce-47641163c837",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(rounded_R_tensor_list)):\n",
    "    target_save_R_tensor = rounded_R_tensor_list[i]\n",
    "    if target==\"p2\":\n",
    "        torch.save(target_save_R_tensor, \"./R_tensor_rounded_noself_p20613\"+raw_tras_name+str(i)+\"_.pt\")\n",
    "    else:\n",
    "        torch.save(target_save_R_tensor, \"./R_tensor_rounded_noself\"+raw_tras_name+str(i)+\"_.pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb431463-a4b1-42dd-b51c-ab8ea32d9a2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff002cc-8039-4f1f-bc3b-7139c24f09b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": true,
  "vscode": {
   "interpreter": {
    "hash": "2f394aca7ca06fed1e6064aef884364492d7cdda3614a461e02e6407fc40ba69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
